{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Milestone 1.ipynb","provenance":[{"file_id":"1QnC7lV7oVFk5OZCm75fqbLAfD9qBy9bw","timestamp":1650728036958}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"10562de6ea2640208b1d0db16fe2b8a0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7f3a5606097e4f7ab466a907586810b2","IPY_MODEL_0033a45354834dcd8dc963974296c8f5","IPY_MODEL_c9f4523bdfe74f938c12c4b04da31aa9"],"layout":"IPY_MODEL_9f92bd51710549b5b90f9d195b2367af"}},"7f3a5606097e4f7ab466a907586810b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f58fa6ead8ea45849a3202c48bccfe9c","placeholder":"‚Äã","style":"IPY_MODEL_277f6a57f1d84305bc2503a64f1ff7b0","value":"100%"}},"0033a45354834dcd8dc963974296c8f5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_42fc151aae264eccaa8870e343b123d2","max":4062133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e46d09a2c68f468d9a4a41183bd15f7f","value":4062133}},"c9f4523bdfe74f938c12c4b04da31aa9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ac5521eea0e490ba6853231e02e2536","placeholder":"‚Äã","style":"IPY_MODEL_b19138aafa4d45598c7d4bc4241804bc","value":" 3.87M/3.87M [00:00&lt;00:00, 44.5MB/s]"}},"9f92bd51710549b5b90f9d195b2367af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f58fa6ead8ea45849a3202c48bccfe9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"277f6a57f1d84305bc2503a64f1ff7b0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42fc151aae264eccaa8870e343b123d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e46d09a2c68f468d9a4a41183bd15f7f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5ac5521eea0e490ba6853231e02e2536":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b19138aafa4d45598c7d4bc4241804bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"jzRKNe1iSlNW"},"source":["# Google Colab: Access Webcam for Images and Video\n","This notebook will go through how to access and run code on images and video taken using your webcam.  \n","\n","For this purpose of this tutorial we will be using OpenCV's Haar Cascade to do face detection on our Webcam image and video."]},{"cell_type":"code","source":["!pip install mediapipe PyYAML"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FeoDS0E12IEK","executionInfo":{"status":"ok","timestamp":1650884954383,"user_tz":-120,"elapsed":11996,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"}},"outputId":"6f8d1f8a-d822-4280-d38c-7154bfc9645f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mediapipe\n","  Downloading mediapipe-0.8.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.7 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32.7 MB 185 kB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (3.13)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.21.6)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (21.4.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.0.0)\n","Requirement already satisfied: protobuf>=3.11.4 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.1.2.30)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.11.4->mediapipe) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.4.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (3.0.8)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mediapipe) (4.1.1)\n","Installing collected packages: mediapipe\n","Successfully installed mediapipe-0.8.9.1\n"]}]},{"cell_type":"code","metadata":{"id":"Fj9YcAnsT4B_","executionInfo":{"status":"ok","timestamp":1650884965296,"user_tz":-120,"elapsed":10933,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"}}},"source":["# import dependencies\n","from IPython.display import display, Javascript, Image\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","import cv2\n","import numpy as np\n","import PIL\n","import io\n","import html\n","import time\n","\n","###\n","\n","import mediapipe as mp\n","import tensorflow as tf\n","import time\n","import copy\n","import itertools\n","import torch\n","\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"11HqzzfG-EmW","executionInfo":{"status":"ok","timestamp":1650884988154,"user_tz":-120,"elapsed":22861,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"}},"outputId":"0a647046-2241-4584-9948-1a5328eef208"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["yolo = torch.hub.load('ultralytics/yolov5', 'yolov5n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":414,"referenced_widgets":["10562de6ea2640208b1d0db16fe2b8a0","7f3a5606097e4f7ab466a907586810b2","0033a45354834dcd8dc963974296c8f5","c9f4523bdfe74f938c12c4b04da31aa9","9f92bd51710549b5b90f9d195b2367af","f58fa6ead8ea45849a3202c48bccfe9c","277f6a57f1d84305bc2503a64f1ff7b0","42fc151aae264eccaa8870e343b123d2","e46d09a2c68f468d9a4a41183bd15f7f","5ac5521eea0e490ba6853231e02e2536","b19138aafa4d45598c7d4bc4241804bc"]},"id":"havEtVK82i-F","executionInfo":{"status":"ok","timestamp":1650885005729,"user_tz":-120,"elapsed":17590,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"}},"outputId":"37a42aef-4edc-4f0d-b1a4-83c67e4cfa62"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n","\u001b[31m\u001b[1mrequirements:\u001b[0m PyYAML>=5.3.1 not found and is required by YOLOv5, attempting auto-update...\n","Collecting PyYAML>=5.3.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","Installing collected packages: PyYAML\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-6.0\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /root/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt\n","\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","YOLOv5 üöÄ 2022-4-25 torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)\n","\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5n.pt to yolov5n.pt...\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/3.87M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10562de6ea2640208b1d0db16fe2b8a0"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\n","Fusing layers... \n","YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n","Adding AutoShape... \n"]}]},{"cell_type":"markdown","metadata":{"id":"L6pCmkJrUC9g"},"source":["## Helper Functions\n","Below are a few helper function such as for the Tello DJI module, drawing bounding boxes, converting between different image data types and formats and to create the webcam video stream using javascript. "]},{"cell_type":"code","source":["mp_drawing = mp.solutions.drawing_utils\n","mp_drawing_styles = mp.solutions.drawing_styles\n","mp_hands = mp.solutions.hands\n","mp_pose = mp.solutions.pose"],"metadata":{"id":"uQ-q9Et14wId","executionInfo":{"status":"ok","timestamp":1650885006240,"user_tz":-120,"elapsed":515,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class KeyPointClassifier(object):\n","    \"\"\"\n","    Classify hand keys points into 8 gestures\n","    \n","    Note: the classification model and class has been taken and refactored from https://github.com/kinivi/tello-gesture-control\n","    \"\"\"\n","    def __init__(\n","        self,\n","        model_path=\"drive/MyDrive/Colab Notebooks/Milestone 1/models/keypoint_classifier.tflite\", # model_path=\"models/keypoint_classifier.tflite\"\n","        \n","    ):\n","        self.interpreter = tf.lite.Interpreter(model_path=model_path)\n","        self.interpreter.allocate_tensors()\n","        self.input_details = self.interpreter.get_input_details()\n","        self.output_details = self.interpreter.get_output_details()\n","\n","    def __call__(\n","        self,\n","        frame,\n","        hand_landmarks,\n","    ):\n","        # Landmark calculation\n","        landmark_list = self._calc_landmark_list(frame, hand_landmarks)\n","\n","        # Conversion to relative coordinates / normalized coordinates\n","        pre_processed_landmark_list = self._pre_process_landmark(landmark_list)\n","\n","        input_details_tensor_index = self.input_details[0]['index']\n","        self.interpreter.set_tensor(\n","            input_details_tensor_index,\n","            np.array([pre_processed_landmark_list], dtype=np.float32))\n","        self.interpreter.invoke()\n","\n","        output_details_tensor_index = self.output_details[0]['index']\n","\n","        result = self.interpreter.get_tensor(output_details_tensor_index)\n","\n","        result_index = np.argmax(np.squeeze(result))\n","\n","        return result_index\n","    \n","    def _pre_process_landmark(self, landmark_list):\n","        temp_landmark_list = copy.deepcopy(landmark_list)\n","\n","        # Convert to relative coordinates\n","        base_x, base_y = 0, 0\n","        for index, landmark_point in enumerate(temp_landmark_list):\n","            if index == 0:\n","                base_x, base_y = landmark_point[0], landmark_point[1]\n","\n","            temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n","            temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n","\n","        # Convert to a one-dimensional list\n","        temp_landmark_list = list(\n","            itertools.chain.from_iterable(temp_landmark_list))\n","\n","        # Normalization\n","        max_value = max(list(map(abs, temp_landmark_list)))\n","\n","        def normalize_(n):\n","            return n / max_value\n","\n","        temp_landmark_list = list(map(normalize_, temp_landmark_list))\n","\n","        return temp_landmark_list\n","    \n","    def _calc_landmark_list(self, image, landmarks):\n","            image_width, image_height = image.shape[1], image.shape[0]\n","\n","            landmark_point = []\n","\n","            # Keypoint\n","            for _, landmark in enumerate(landmarks.landmark):\n","                landmark_x = min(int(landmark.x * image_width), image_width - 1)\n","                landmark_y = min(int(landmark.y * image_height), image_height - 1)\n","                # landmark_z = landmark.z\n","\n","                landmark_point.append([landmark_x, landmark_y])\n","\n","            return landmark_point"],"metadata":{"id":"EH43_pOL40h2","executionInfo":{"status":"ok","timestamp":1650885006241,"user_tz":-120,"elapsed":5,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def calc_bounding_rect(image, landmarks):\n","    image_width, image_height = image.shape[1], image.shape[0]\n","\n","    landmark_array = np.empty((0, 2), int)\n","\n","    for _, landmark in enumerate(landmarks.landmark):\n","        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n","        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n","\n","        landmark_point = [np.array((landmark_x, landmark_y))]\n","\n","        landmark_array = np.append(landmark_array, landmark_point, axis=0)\n","\n","    x, y, w, h = cv2.boundingRect(landmark_array)\n","\n","    return [x, y, x + w, y + h]\n","\n","def calc_center(brect):\n","    return (brect[0] + brect[2]) / 2, (brect[1] + brect[3]) / 2\n","\n","def calc_person_center(p):\n","    return (int(p.xmin) + int(p.xmax)) / 2, (int(p.ymin) + int(p.ymax)) / 2\n","\n","def euclidian_distance(p1, p2):\n","    return np.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)\n","\n","def draw_info(image, brect, hand_sign_text = \"\"):\n","    # Outer rectangle\n","    cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[3]),\n","                    (0, 0, 0), 3)\n","\n","    # Text\n","    cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[1] - 22),\n","                     (0, 0, 0), -1)\n","    cv2.putText(image, hand_sign_text, (brect[0] + 5, brect[1] - 4),\n","                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n","\n","# def draw_person(image, p):\n","#     cv2.rectangle(image, (int(p.xmin), int(p.ymin)), (int(p.xmax), int(p.ymax)), (0, 0, 255), 2)"],"metadata":{"id":"qQeLKSqm454H","executionInfo":{"status":"ok","timestamp":1650885006241,"user_tz":-120,"elapsed":5,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"09b_0FAnUa9y","executionInfo":{"status":"ok","timestamp":1650885006241,"user_tz":-120,"elapsed":4,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"}}},"source":["# function to convert the JavaScript object into an OpenCV image\n","def js_to_image(js_reply):\n","  \"\"\"\n","  Params:\n","          js_reply: JavaScript object containing image from webcam\n","  Returns:\n","          img: OpenCV BGR image\n","  \"\"\"\n","  # decode base64 image\n","  image_bytes = b64decode(js_reply.split(',')[1])\n","  # convert bytes to numpy array\n","  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n","  # decode numpy array into OpenCV BGR image\n","  img = cv2.imdecode(jpg_as_np, flags=1)\n","\n","  return img\n","\n","# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n","def bbox_to_bytes(bbox_array):\n","  \"\"\"\n","  Params:\n","          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n","  Returns:\n","        bytes: Base64 image byte string\n","  \"\"\"\n","  # convert array into PIL image\n","  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n","  iobuf = io.BytesIO()\n","  # format bbox into png for return\n","  bbox_PIL.save(iobuf, format='png')\n","  # format return string\n","  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n","\n","  return bbox_bytes"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ghUlAJzKSjFT","executionInfo":{"status":"ok","timestamp":1650885171452,"user_tz":-120,"elapsed":283,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"}}},"source":["# JavaScript to properly create our live video stream using our webcam as input\n","def video_stream():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth; 640\n","      captureCanvas.height = 480; //video.videoHeight; 480\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    ''')\n","\n","  display(js)\n","  \n","def video_frame(label, bbox):\n","  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n","  return data"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I_xcqQZKYzAj"},"source":["## Resulting Milestone 1\n","Below is the result of what we have done for Milestone 1."]},{"cell_type":"code","metadata":{"id":"1nkSnkbkk4cC","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1650885200514,"user_tz":-120,"elapsed":27328,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"}},"outputId":"f0523f2d-a8d5-40a1-9c37-58cb21b78d7e"},"source":["# start streaming video from webcam\n","video_stream()\n","# label for video\n","label_html = 'Capturing...'\n","# initialze bounding box to empty\n","bbox = ''\n","count = 0 \n","while True:\n","    js_reply = video_frame(label_html, bbox)\n","    if not js_reply:\n","        break\n","\n","    # convert JS response to OpenCV Image\n","    img = js_to_image(js_reply[\"img\"])\n","\n","    # create transparent overlay for bounding box\n","    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n","\n","    key_point_classifier = KeyPointClassifier()\n","    with mp_hands.Hands(\n","      model_complexity=0,\n","      min_detection_confidence=0.5,\n","      min_tracking_confidence=0.5,\n","      max_num_hands=4) as hands:\n","\n","      # img = cv2.flip(img, 1)\n","\n","      # To improve performance, optionally mark the image as not writeable to\n","      # pass by reference.\n","      img.flags.writeable = False\n","      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","      results = hands.process(img)\n","      \n","      # Draw the hand annotations on the image.\n","      img.flags.writeable = True\n","      img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n","\n","      yolo_pred = yolo(img)\n","      persons = yolo_pred.pandas().xyxy[0]\n","      persons = persons[persons['class'] == 0]\n","\n","      if results.multi_hand_landmarks:\n","        hand_center = (-1, -1)\n","        for hand_landmarks in results.multi_hand_landmarks:\n","          mp_drawing.draw_landmarks(\n","            img,\n","            hand_landmarks,\n","            mp_hands.HAND_CONNECTIONS,\n","            mp_drawing_styles.get_default_hand_landmarks_style(),\n","            mp_drawing_styles.get_default_hand_connections_style())\n","          \n","          gesture_idx = key_point_classifier(img, hand_landmarks)\n","          if gesture_idx == 2:\n","            brect = calc_bounding_rect(img, hand_landmarks)\n","            draw_info(img, brect, \"Sign detected\")\n","            hand_center = calc_center(calc_bounding_rect(img, hand_landmarks))\n","\n","        if hand_center != (-1, -1):\n","          distances = []\n","          for _, p in persons.iterrows():\n","            distances.append(euclidian_distance(calc_person_center(p), hand_center))\n","          if len(distances) > 0:\n","            min_idx = np.argmin(distances)\n","            # draw_person(image, persons.iloc[min_idx])\n","            p = persons.iloc[min_idx]\n","            bbox_array = cv2.rectangle(bbox_array,(int(p.xmin), int(p.ymin)), (int(p.xmax), int(p.ymax)),(255,0,0),2)\n","\n","    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n","    # convert overlay of bbox into bytes\n","    bbox_bytes = bbox_to_bytes(bbox_array)\n","    # update bbox so next frame gets new overlay\n","    bbox = bbox_bytes"],"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 600; //video.videoWidth; 640\n","      captureCanvas.height = 600; //video.videoHeight; 480\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    "]},"metadata":{}}]},{"cell_type":"code","source":[""],"metadata":{"id":"3e4NrNs4GTtW","executionInfo":{"status":"ok","timestamp":1650885050333,"user_tz":-120,"elapsed":10,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"}}},"execution_count":10,"outputs":[]}]}