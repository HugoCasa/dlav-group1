{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJfKZ-iqwxLh",
        "outputId": "4cf628e7-52fb-40b9-c2f8-af73a63da34c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving folder list\n",
            "Processing file 1-H9MbKRSD4wKBbTYhvcTYXXjN9tC3FDJ keypoint_classifier.tflite\n",
            "Processing file 112vWBajDQTK7xqMslPae9U1LVSutjKFA nanodet_m_0.5x.ckpt\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-H9MbKRSD4wKBbTYhvcTYXXjN9tC3FDJ\n",
            "To: /content/models/keypoint_classifier.tflite\n",
            "100% 7.84k/7.84k [00:00<00:00, 13.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=112vWBajDQTK7xqMslPae9U1LVSutjKFA\n",
            "To: /content/models/nanodet_m_0.5x.ckpt\n",
            "100% 1.25M/1.25M [00:00<00:00, 110MB/s]\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "# download pretrained weights\n",
        "!gdown --folder 1nPYVv-UD2qPDStWy0hHVGPcBzDGbW2r6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BVc6B7PWROzm",
        "outputId": "50999fc3-c3b9-408d-c24a-e0f2b411ac54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'nanodet'...\n",
            "remote: Enumerating objects: 2511, done.\u001b[K\n",
            "remote: Total 2511 (delta 0), reused 0 (delta 0), pack-reused 2511\u001b[K\n",
            "Receiving objects: 100% (2511/2511), 5.23 MiB | 12.46 MiB/s, done.\n",
            "Resolving deltas: 100% (1469/1469), done.\n",
            "/Users/hugo/projects/dlav/nanodet\n",
            "Requirement already satisfied: Cython in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (0.29.28)\n",
            "Requirement already satisfied: matplotlib in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (3.5.1)\n",
            "Requirement already satisfied: numpy in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.21.6)\n",
            "Requirement already satisfied: omegaconf>=2.0.1 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (2.1.2)\n",
            "Requirement already satisfied: onnx in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (1.11.0)\n",
            "Requirement already satisfied: onnx-simplifier in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (0.3.7)\n",
            "Requirement already satisfied: opencv-python in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (4.5.5.64)\n",
            "Requirement already satisfied: pyaml in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (21.10.1)\n",
            "Requirement already satisfied: pycocotools in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from -r requirements.txt (line 9)) (2.0.4)\n",
            "Requirement already satisfied: pytorch-lightning>=1.4.0 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (1.6.1)\n",
            "Requirement already satisfied: tabulate in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from -r requirements.txt (line 11)) (0.8.9)\n",
            "Requirement already satisfied: tensorboard in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from -r requirements.txt (line 12)) (2.8.0)\n",
            "Requirement already satisfied: termcolor in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from -r requirements.txt (line 13)) (1.1.0)\n",
            "Requirement already satisfied: torch>=1.7 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from -r requirements.txt (line 14)) (1.11.0)\n",
            "Requirement already satisfied: torchmetrics in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from -r requirements.txt (line 15)) (0.8.0)\n",
            "Requirement already satisfied: torchvision in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from -r requirements.txt (line 16)) (0.12.0)\n",
            "Requirement already satisfied: tqdm in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from -r requirements.txt (line 17)) (4.63.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 2)) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 2)) (9.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 2)) (3.0.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 2)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 2)) (4.33.2)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from omegaconf>=2.0.1->-r requirements.txt (line 4)) (6.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from omegaconf>=2.0.1->-r requirements.txt (line 4)) (4.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from onnx->-r requirements.txt (line 5)) (4.2.0)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from onnx->-r requirements.txt (line 5)) (3.20.1)\n",
            "Requirement already satisfied: onnxruntime>=1.6.0 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from onnx-simplifier->-r requirements.txt (line 6)) (1.11.0)\n",
            "Requirement already satisfied: onnxoptimizer>=0.2.5 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from onnx-simplifier->-r requirements.txt (line 6)) (0.2.6)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from pytorch-lightning>=1.4.0->-r requirements.txt (line 10)) (2022.3.0)\n",
            "Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from pytorch-lightning>=1.4.0->-r requirements.txt (line 10)) (0.3.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from tensorboard->-r requirements.txt (line 12)) (1.37.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from tensorboard->-r requirements.txt (line 12)) (2.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from tensorboard->-r requirements.txt (line 12)) (62.1.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from tensorboard->-r requirements.txt (line 12)) (1.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from tensorboard->-r requirements.txt (line 12)) (0.15.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from tensorboard->-r requirements.txt (line 12)) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from tensorboard->-r requirements.txt (line 12)) (2.6.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from tensorboard->-r requirements.txt (line 12)) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from tensorboard->-r requirements.txt (line 12)) (3.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from tensorboard->-r requirements.txt (line 12)) (0.37.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from tensorboard->-r requirements.txt (line 12)) (2.1.1)\n",
            "Requirement already satisfied: six in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from absl-py>=0.4->tensorboard->-r requirements.txt (line 12)) (1.16.0)\n",
            "Requirement already satisfied: aiohttp in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->-r requirements.txt (line 10)) (3.7.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 12)) (4.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 12)) (5.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 12)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 12)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 12)) (4.11.3)\n",
            "Requirement already satisfied: flatbuffers in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from onnxruntime>=1.6.0->onnx-simplifier->-r requirements.txt (line 6)) (1.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 12)) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 12)) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 12)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 12)) (1.26.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 12)) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 12)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 12)) (3.2.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->-r requirements.txt (line 10)) (3.0.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->-r requirements.txt (line 10)) (6.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->-r requirements.txt (line 10)) (21.4.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->-r requirements.txt (line 10)) (1.6.0)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->-r requirements.txt (line 10)) (3.0.4)\n",
            "/Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages/setuptools/dist.py:516: UserWarning: Normalizing '1.0.0-alpha' to '1.0.0a0'\n",
            "  warnings.warn(tmpl.format(**locals()))\n",
            "running develop\n",
            "/Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages/setuptools/command/easy_install.py:147: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
            "  EasyInstallDeprecationWarning,\n",
            "/Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
            "  setuptools.SetuptoolsDeprecationWarning,\n",
            "running egg_info\n",
            "creating nanodet.egg-info\n",
            "writing nanodet.egg-info/PKG-INFO\n",
            "writing dependency_links to nanodet.egg-info/dependency_links.txt\n",
            "writing top-level names to nanodet.egg-info/top_level.txt\n",
            "writing manifest file 'nanodet.egg-info/SOURCES.txt'\n",
            "reading manifest file 'nanodet.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'nanodet.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /Users/hugo/opt/anaconda3/envs/dlav/lib/python3.7/site-packages/nanodet.egg-link (link to .)\n",
            "nanodet 1.0.0a0 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /Users/hugo/projects/dlav/nanodet\n",
            "Processing dependencies for nanodet==1.0.0a0\n",
            "Finished processing dependencies for nanodet==1.0.0a0\n"
          ]
        }
      ],
      "source": [
        "# clone nanodet repo\n",
        "!git clone https://github.com/RangiLyu/nanodet.git\n",
        "# install nanodet requirements\n",
        "%cd nanodet\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeoDS0E12IEK",
        "outputId": "200cbccb-5826-45b8-b4d1-d2c98a96cf32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.8.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.7 MB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.21.6)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (21.4.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.1.2.30)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n",
            "Requirement already satisfied: protobuf>=3.11.4 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.11.4->mediapipe) (1.15.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (3.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mediapipe) (4.2.0)\n",
            "Installing collected packages: mediapipe\n",
            "Successfully installed mediapipe-0.8.9.1\n"
          ]
        }
      ],
      "source": [
        "# install other missing dep\n",
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Fj9YcAnsT4B_"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/2d/qlljx9fd3_vckd2wfbdrtq500000gn/T/ipykernel_33499/1644701484.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJavascript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbase64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mb64decode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb64encode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "import os\n",
        "\n",
        "###\n",
        "\n",
        "import mediapipe as mp\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import copy\n",
        "import itertools\n",
        "import torch\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6pCmkJrUC9g"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uQ-q9Et14wId"
      },
      "outputs": [],
      "source": [
        "# load media pipe hand detection model\n",
        "mp_hands = mp.solutions.hands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EH43_pOL40h2"
      },
      "outputs": [],
      "source": [
        "# load hand keypoints classifier (detect sign)\n",
        "model_path = '../models/keypoint_classifier.tflite'\n",
        "class KeyPointClassifier(object):\n",
        "    \"\"\"\n",
        "    Classify hand keys points into 8 gestures\n",
        "    \n",
        "    Note: the classification model and class has been taken and refactored from https://github.com/kinivi/tello-gesture-control\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path=\"models/keypoint_classifier.tflite\"\n",
        "        \n",
        "    ):\n",
        "        self.interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "        self.interpreter.allocate_tensors()\n",
        "        self.input_details = self.interpreter.get_input_details()\n",
        "        self.output_details = self.interpreter.get_output_details()\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        frame,\n",
        "        hand_landmarks,\n",
        "    ):\n",
        "        # Landmark calculation\n",
        "        landmark_list = self._calc_landmark_list(frame, hand_landmarks)\n",
        "\n",
        "        # Conversion to relative coordinates / normalized coordinates\n",
        "        pre_processed_landmark_list = self._pre_process_landmark(landmark_list)\n",
        "\n",
        "        input_details_tensor_index = self.input_details[0]['index']\n",
        "        self.interpreter.set_tensor(\n",
        "            input_details_tensor_index,\n",
        "            np.array([pre_processed_landmark_list], dtype=np.float32))\n",
        "        self.interpreter.invoke()\n",
        "\n",
        "        output_details_tensor_index = self.output_details[0]['index']\n",
        "\n",
        "        result = self.interpreter.get_tensor(output_details_tensor_index)\n",
        "\n",
        "        result_index = np.argmax(np.squeeze(result))\n",
        "\n",
        "        return result_index\n",
        "    \n",
        "    def _pre_process_landmark(self, landmark_list):\n",
        "        temp_landmark_list = copy.deepcopy(landmark_list)\n",
        "\n",
        "        # Convert to relative coordinates\n",
        "        base_x, base_y = 0, 0\n",
        "        for index, landmark_point in enumerate(temp_landmark_list):\n",
        "            if index == 0:\n",
        "                base_x, base_y = landmark_point[0], landmark_point[1]\n",
        "\n",
        "            temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
        "            temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
        "\n",
        "        # Convert to a one-dimensional list\n",
        "        temp_landmark_list = list(\n",
        "            itertools.chain.from_iterable(temp_landmark_list))\n",
        "\n",
        "        # Normalization\n",
        "        max_value = max(list(map(abs, temp_landmark_list)))\n",
        "\n",
        "        def normalize_(n):\n",
        "            return n / max_value\n",
        "\n",
        "        temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
        "\n",
        "        return temp_landmark_list\n",
        "    \n",
        "    def _calc_landmark_list(self, image, landmarks):\n",
        "            image_width, image_height = image.shape[1], image.shape[0]\n",
        "\n",
        "            landmark_point = []\n",
        "\n",
        "            # Keypoint\n",
        "            for _, landmark in enumerate(landmarks.landmark):\n",
        "                landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
        "                landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
        "                # landmark_z = landmark.z\n",
        "\n",
        "                landmark_point.append([landmark_x, landmark_y])\n",
        "\n",
        "            return landmark_point\n",
        "\n",
        "# initialize hand keypoint classifier\n",
        "key_point_classifier = KeyPointClassifier(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161,
          "referenced_widgets": [
            "538dd86b22934fd7a2d8fb2082059f1a",
            "f7333681e41a4e5482e7e1262b3910af",
            "88496c6664a548ea8a3875bd4c1d4457",
            "e0cacaf5f4684d8297c6a4ef41876096",
            "25a694878a524d4e84a4becfde9c47a7",
            "07d33aafd2f34432aa449fa3f79615a4",
            "841dbefcd3e040c4996f7d0c9d56e1df",
            "3bc4821395be4757a68fde883f45c778",
            "5cc0f2aaa8b34c489d978045e93941a5",
            "50d8be2ad8ce469faa16a1a3f8062bd2",
            "5ae72d1a9dbb4e49a136424ad155471d"
          ]
        },
        "id": "siK8itawSPeg",
        "outputId": "7f4ba6fc-4b2b-48ea-d5b6-938150d219d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model size is  0.5x\n",
            "init weights...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/shufflenetv2_x0.5-f707e7126e.pth\" to /root/.cache/torch/hub/checkpoints/shufflenetv2_x0.5-f707e7126e.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "538dd86b22934fd7a2d8fb2082059f1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/5.28M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> loading pretrained model https://download.pytorch.org/models/shufflenetv2_x0.5-f707e7126e.pth\n",
            "Finish initialize NanoDet Head.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# load nanodet model (for person detection) and use cuda if available\n",
        "from nanodet.util import Logger, cfg, load_config\n",
        "from demo.demo import Predictor\n",
        "load_config(cfg, 'config/legacy_v0.x_configs/nanodet-m-0.5x.yml')\n",
        "logger = Logger(0, use_tensorboard=False)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "predictor = Predictor(cfg, '../models/nanodet_m_0.5x.ckpt', logger, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVSGnYcgRGCa"
      },
      "source": [
        "## Helper Functions\n",
        "Below are a few helper converting between different image data types and formats and to create the webcam video stream using javascript. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qQeLKSqm454H"
      },
      "outputs": [],
      "source": [
        "def calc_bounding_rect(image, landmarks):\n",
        "    # Calculate bounding box from hand landmarks\n",
        "    image_width, image_height = image.shape[1], image.shape[0]\n",
        "\n",
        "    landmark_array = np.empty((0, 2), int)\n",
        "\n",
        "    for _, landmark in enumerate(landmarks.landmark):\n",
        "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
        "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
        "\n",
        "        landmark_point = [np.array((landmark_x, landmark_y))]\n",
        "\n",
        "        landmark_array = np.append(landmark_array, landmark_point, axis=0)\n",
        "\n",
        "    x, y, w, h = cv2.boundingRect(landmark_array)\n",
        "\n",
        "    return [x, y, x + w, y + h]\n",
        "\n",
        "def calc_center(brect):\n",
        "    # calculate center of hand bounding box\n",
        "    return (brect[0] + brect[2]) / 2, (brect[1] + brect[3]) / 2\n",
        "\n",
        "def draw_info(image, brect, hand_sign_text = \"\"):\n",
        "    # draw bounding box of hand\n",
        "    # Outer rectangle\n",
        "    cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[3]),\n",
        "                    (255, 0, 0), 2)\n",
        "\n",
        "    # Text\n",
        "    cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[1] - 22),\n",
        "                     (255, 0, 0), -1)\n",
        "    cv2.putText(image, hand_sign_text, (brect[0] + 5, brect[1] - 4),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
        "\n",
        "def calc_person_center(box):\n",
        "    # calculate center of person bouding box\n",
        "    x0 = int(box[0])\n",
        "    y0 = int(box[1])\n",
        "    x1 = int(box[2])\n",
        "    y1 = int(box[3])\n",
        "    # calculate center of person bouding box\n",
        "    return (x0 + x1) / 2, (y0 + y1) / 2\n",
        "\n",
        "def draw_person(image, box):\n",
        "    # draw person bounding box\n",
        "    x0 = int(box[0])\n",
        "    y0 = int(box[1])\n",
        "    x1 = int(box[2])\n",
        "    y1 = int(box[3])\n",
        "\n",
        "    color = (0, 0, 255)\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    cv2.rectangle(image, (x0, y0), (x1, y1), color, 2)\n",
        "\n",
        "def in_bounding_box(p, bbox):\n",
        "    # check if point is in bounding box\n",
        "    return p[0] >= bbox[0] and p[0] <= bbox[2] and p[1] >= bbox[1] and p[1] <= bbox[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "09b_0FAnUa9y"
      },
      "outputs": [],
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ghUlAJzKSjFT"
      },
      "outputs": [],
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth; 640\n",
        "      captureCanvas.height = 480; //video.videoHeight; 480\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_xcqQZKYzAj"
      },
      "source": [
        "## Resulting Milestone 1\n",
        "Below is the result of what we have done for Milestone 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "id": "1nkSnkbkk4cC",
        "outputId": "f6bcfb4f-bc1e-4ab7-d2f4-8c77605953db"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    var video;\n    var div = null;\n    var stream;\n    var captureCanvas;\n    var imgElement;\n    var labelElement;\n    \n    var pendingResolve = null;\n    var shutdown = false;\n    \n    function removeDom() {\n       stream.getVideoTracks()[0].stop();\n       video.remove();\n       div.remove();\n       video = null;\n       div = null;\n       stream = null;\n       imgElement = null;\n       captureCanvas = null;\n       labelElement = null;\n    }\n    \n    function onAnimationFrame() {\n      if (!shutdown) {\n        window.requestAnimationFrame(onAnimationFrame);\n      }\n      if (pendingResolve) {\n        var result = \"\";\n        if (!shutdown) {\n          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n        }\n        var lp = pendingResolve;\n        pendingResolve = null;\n        lp(result);\n      }\n    }\n    \n    async function createDom() {\n      if (div !== null) {\n        return stream;\n      }\n\n      div = document.createElement('div');\n      div.style.border = '2px solid black';\n      div.style.padding = '3px';\n      div.style.width = '100%';\n      div.style.maxWidth = '600px';\n      document.body.appendChild(div);\n      \n      const modelOut = document.createElement('div');\n      modelOut.innerHTML = \"<span>Status:</span>\";\n      labelElement = document.createElement('span');\n      labelElement.innerText = 'No data';\n      labelElement.style.fontWeight = 'bold';\n      modelOut.appendChild(labelElement);\n      div.appendChild(modelOut);\n           \n      video = document.createElement('video');\n      video.style.display = 'block';\n      video.width = div.clientWidth - 6;\n      video.setAttribute('playsinline', '');\n      video.onclick = () => { shutdown = true; };\n      stream = await navigator.mediaDevices.getUserMedia(\n          {video: { facingMode: \"environment\"}});\n      div.appendChild(video);\n\n      imgElement = document.createElement('img');\n      imgElement.style.position = 'absolute';\n      imgElement.style.zIndex = 1;\n      imgElement.onclick = () => { shutdown = true; };\n      div.appendChild(imgElement);\n      \n      const instruction = document.createElement('div');\n      instruction.innerHTML = \n          '<span style=\"color: red; font-weight: bold;\">' +\n          'When finished, click here or on the video to stop this demo</span>';\n      div.appendChild(instruction);\n      instruction.onclick = () => { shutdown = true; };\n      \n      video.srcObject = stream;\n      await video.play();\n\n      captureCanvas = document.createElement('canvas');\n      captureCanvas.width = 640; //video.videoWidth; 640\n      captureCanvas.height = 480; //video.videoHeight; 480\n      window.requestAnimationFrame(onAnimationFrame);\n      \n      return stream;\n    }\n    async function stream_frame(label, imgData) {\n      if (shutdown) {\n        removeDom();\n        shutdown = false;\n        return '';\n      }\n\n      var preCreate = Date.now();\n      stream = await createDom();\n      \n      var preShow = Date.now();\n      if (label != \"\") {\n        labelElement.innerHTML = label;\n      }\n            \n      if (imgData != \"\") {\n        var videoRect = video.getClientRects()[0];\n        imgElement.style.top = videoRect.top + \"px\";\n        imgElement.style.left = videoRect.left + \"px\";\n        imgElement.style.width = videoRect.width + \"px\";\n        imgElement.style.height = videoRect.height + \"px\";\n        imgElement.src = imgData;\n      }\n      \n      var preCapture = Date.now();\n      var result = await new Promise(function(resolve, reject) {\n        pendingResolve = resolve;\n      });\n      shutdown = false;\n      \n      return {'create': preShow - preCreate, \n              'show': preCapture - preShow, \n              'capture': Date.now() - preCapture,\n              'img': result};\n    }\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "forward time: 0.015s | decode time: 0.008s | forward time: 0.015s | decode time: 0.007s | forward time: 0.014s | decode time: 0.007s | forward time: 0.015s | decode time: 0.007s | forward time: 0.015s | decode time: 0.008s | forward time: 0.020s | decode time: 0.008s | forward time: 0.015s | decode time: 0.007s | forward time: 0.015s | decode time: 0.008s | forward time: 0.015s | decode time: 0.008s | forward time: 0.015s | decode time: 0.008s | forward time: 0.015s | decode time: 0.008s | forward time: 0.015s | decode time: 0.008s | forward time: 0.015s | decode time: 0.008s | forward time: 0.018s | decode time: 0.007s | forward time: 0.015s | decode time: 0.008s | forward time: 0.015s | decode time: 0.008s | forward time: 0.015s | decode time: 0.011s | forward time: 0.015s | decode time: 0.008s | forward time: 0.014s | decode time: 0.007s | forward time: 0.014s | decode time: 0.008s | forward time: 0.016s | decode time: 0.008s | forward time: 0.017s | decode time: 0.008s | forward time: 0.014s | decode time: 0.007s | forward time: 0.015s | decode time: 0.008s | forward time: 0.022s | decode time: 0.009s | forward time: 0.014s | decode time: 0.007s | forward time: 0.014s | decode time: 0.007s | forward time: 0.015s | decode time: 0.007s | forward time: 0.015s | decode time: 0.008s | forward time: 0.015s | decode time: 0.008s | forward time: 0.017s | decode time: 0.008s | forward time: 0.015s | decode time: 0.008s | forward time: 0.017s | decode time: 0.007s | forward time: 0.015s | decode time: 0.008s | forward time: 0.014s | decode time: 0.008s | "
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-d10086d372f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     max_num_hands=4) as hands:\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mjs_reply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_html\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjs_reply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-2de0ad1ab8d2>\u001b[0m in \u001b[0;36mvideo_frame\u001b[0;34m(label, bbox)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvideo_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stream_frame(\"{}\", \"{}\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "\n",
        "with mp_hands.Hands(\n",
        "    model_complexity=0,\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5,\n",
        "    max_num_hands=4) as hands:\n",
        "  while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    # Process the image in a non-writable way (faster) + convert to RGB to detect hands\n",
        "    img_copy = img.copy()\n",
        "    img_copy.flags.writeable = False\n",
        "    img_copy = cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB)\n",
        "    results = hands.process(img_copy)\n",
        "\n",
        "    # if hands detected\n",
        "    if results.multi_hand_landmarks:\n",
        "      hand_centers = []\n",
        "      # for each hand\n",
        "      for hand_landmarks in results.multi_hand_landmarks:\n",
        "        # classify hand keypoints\n",
        "        gesture_idx = key_point_classifier(img, hand_landmarks)\n",
        "        # if sign detected (== 2, \"up\")\n",
        "        if gesture_idx == 2:\n",
        "          # save center of hand bounding box\n",
        "          brect = calc_bounding_rect(img, hand_landmarks)\n",
        "          hand_center = calc_center(brect)\n",
        "          hand_centers.append(hand_center)\n",
        "          # and draw it\n",
        "          draw_info(bbox_array, brect, \"Sign detected\")\n",
        "\n",
        "      # if the sign is detected more than once\n",
        "      if len(hand_centers) > 1:\n",
        "        # detect persons in the frame\n",
        "        meta, res = predictor.inference(img);\n",
        "        dets = res[0]\n",
        "        bboxes = []\n",
        "        distances = []\n",
        "        for label in dets:\n",
        "          # only take persons\n",
        "          if label == 0:\n",
        "            for bbox in dets[label]:\n",
        "              score = bbox[-1]\n",
        "              # take good predictions\n",
        "              if score > 0.35:\n",
        "                rel_hand_centers = []\n",
        "                # check if at least two hands are in the person bounding box\n",
        "                for hand_center in hand_centers:\n",
        "                  if in_bounding_box(hand_center, bbox[:4]):\n",
        "                    rel_hand_centers.append(hand_center)\n",
        "                if len(rel_hand_centers) > 1:\n",
        "                  # if yes, draw the person bounding box\n",
        "                  draw_person(bbox_array, bbox[:4])\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e4NrNs4GTtW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Milestone_1_nanodet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07d33aafd2f34432aa449fa3f79615a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25a694878a524d4e84a4becfde9c47a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bc4821395be4757a68fde883f45c778": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50d8be2ad8ce469faa16a1a3f8062bd2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "538dd86b22934fd7a2d8fb2082059f1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f7333681e41a4e5482e7e1262b3910af",
              "IPY_MODEL_88496c6664a548ea8a3875bd4c1d4457",
              "IPY_MODEL_e0cacaf5f4684d8297c6a4ef41876096"
            ],
            "layout": "IPY_MODEL_25a694878a524d4e84a4becfde9c47a7"
          }
        },
        "5ae72d1a9dbb4e49a136424ad155471d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cc0f2aaa8b34c489d978045e93941a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "841dbefcd3e040c4996f7d0c9d56e1df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88496c6664a548ea8a3875bd4c1d4457": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bc4821395be4757a68fde883f45c778",
            "max": 5538128,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5cc0f2aaa8b34c489d978045e93941a5",
            "value": 5538128
          }
        },
        "e0cacaf5f4684d8297c6a4ef41876096": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50d8be2ad8ce469faa16a1a3f8062bd2",
            "placeholder": "​",
            "style": "IPY_MODEL_5ae72d1a9dbb4e49a136424ad155471d",
            "value": " 5.28M/5.28M [00:00&lt;00:00, 1.95MB/s]"
          }
        },
        "f7333681e41a4e5482e7e1262b3910af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07d33aafd2f34432aa449fa3f79615a4",
            "placeholder": "​",
            "style": "IPY_MODEL_841dbefcd3e040c4996f7d0c9d56e1df",
            "value": "100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
