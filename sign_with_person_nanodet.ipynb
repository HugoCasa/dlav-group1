{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'nanodet-cpu'...\n",
      "remote: Enumerating objects: 2517, done.\u001b[K\n",
      "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
      "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
      "remote: Total 2517 (delta 0), reused 0 (delta 0), pack-reused 2515\u001b[K\n",
      "Receiving objects: 100% (2517/2517), 5.23 MiB | 14.80 MiB/s, done.\n",
      "Resolving deltas: 100% (1472/1472), done.\n",
      "/Users/hugo/projects/dlav/nanodet-cpu\n",
      "/Users/hugo/opt/anaconda3/envs/yolox/lib/python3.7/site-packages/setuptools/dist.py:516: UserWarning: Normalizing '1.0.0-alpha' to '1.0.0a0'\n",
      "  warnings.warn(tmpl.format(**locals()))\n",
      "running develop\n",
      "/Users/hugo/opt/anaconda3/envs/yolox/lib/python3.7/site-packages/setuptools/command/easy_install.py:147: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
      "  EasyInstallDeprecationWarning,\n",
      "/Users/hugo/opt/anaconda3/envs/yolox/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n",
      "running egg_info\n",
      "creating nanodet.egg-info\n",
      "writing nanodet.egg-info/PKG-INFO\n",
      "writing dependency_links to nanodet.egg-info/dependency_links.txt\n",
      "writing top-level names to nanodet.egg-info/top_level.txt\n",
      "writing manifest file 'nanodet.egg-info/SOURCES.txt'\n",
      "reading manifest file 'nanodet.egg-info/SOURCES.txt'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'nanodet.egg-info/SOURCES.txt'\n",
      "running build_ext\n",
      "Creating /Users/hugo/opt/anaconda3/envs/yolox/lib/python3.7/site-packages/nanodet.egg-link (link to .)\n",
      "Adding nanodet 1.0.0a0 to easy-install.pth file\n",
      "\n",
      "Installed /Users/hugo/projects/dlav/nanodet-cpu\n",
      "Processing dependencies for nanodet==1.0.0a0\n",
      "Finished processing dependencies for nanodet==1.0.0a0\n"
     ]
    }
   ],
   "source": [
    "# clone nanodet repo (cpu fork)\n",
    "!git clone https://github.com/HugoCasa/nanodet-cpu.git\n",
    "%cd nanodet-cpu\n",
    "# install nanodet requirements\n",
    "#!pip install -r requirements.txt # only need once\n",
    "!python setup.py develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import torch\n",
    "import itertools\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanodet.util import Logger, cfg, load_config\n",
    "from demo.demo import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size is  0.5x\n",
      "init weights...\n",
      "=> loading pretrained model https://download.pytorch.org/models/shufflenetv2_x0.5-f707e7126e.pth\n",
      "Finish initialize NanoDet Head.\n"
     ]
    }
   ],
   "source": [
    "load_config(cfg, 'config/legacy_v0.x_configs/nanodet-m-0.5x.yml')\n",
    "logger = Logger(0, use_tensorboard=False)\n",
    "predictor = Predictor(cfg, '../models/nanodet_m_0.5x.ckpt', None, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load media pipe drawing solutions\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "# load media pipe hand detection model\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Initialized TensorFlow Lite runtime.\n"
     ]
    }
   ],
   "source": [
    "# load hand keypoints classifier\n",
    "model_path = \"../models/keypoint_classifier.tflite\"\n",
    "class KeyPointClassifier(object):\n",
    "    \"\"\"\n",
    "    Classify hand keys points into 8 gestures\n",
    "    \n",
    "    Note: the classification model and class has been taken and refactored from https://github.com/kinivi/tello-gesture-control\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path=\"models/keypoint_classifier.tflite\",\n",
    "        \n",
    "    ):\n",
    "        self.interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "        self.interpreter.allocate_tensors()\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        frame,\n",
    "        hand_landmarks,\n",
    "    ):\n",
    "        # Landmark calculation\n",
    "        landmark_list = self._calc_landmark_list(frame, hand_landmarks)\n",
    "\n",
    "        # Conversion to relative coordinates / normalized coordinates\n",
    "        pre_processed_landmark_list = self._pre_process_landmark(landmark_list)\n",
    "\n",
    "        input_details_tensor_index = self.input_details[0]['index']\n",
    "        self.interpreter.set_tensor(\n",
    "            input_details_tensor_index,\n",
    "            np.array([pre_processed_landmark_list], dtype=np.float32))\n",
    "        self.interpreter.invoke()\n",
    "\n",
    "        output_details_tensor_index = self.output_details[0]['index']\n",
    "\n",
    "        result = self.interpreter.get_tensor(output_details_tensor_index)\n",
    "\n",
    "        result_index = np.argmax(np.squeeze(result))\n",
    "\n",
    "        return result_index\n",
    "    \n",
    "    def _pre_process_landmark(self, landmark_list):\n",
    "        temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "\n",
    "        # Convert to relative coordinates\n",
    "        base_x, base_y = 0, 0\n",
    "        for index, landmark_point in enumerate(temp_landmark_list):\n",
    "            if index == 0:\n",
    "                base_x, base_y = landmark_point[0], landmark_point[1]\n",
    "\n",
    "            temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
    "            temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
    "\n",
    "        # Convert to a one-dimensional list\n",
    "        temp_landmark_list = list(\n",
    "            itertools.chain.from_iterable(temp_landmark_list))\n",
    "\n",
    "        # Normalization\n",
    "        max_value = max(list(map(abs, temp_landmark_list)))\n",
    "\n",
    "        def normalize_(n):\n",
    "            return n / max_value\n",
    "\n",
    "        temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
    "\n",
    "        return temp_landmark_list\n",
    "    \n",
    "    def _calc_landmark_list(self, image, landmarks):\n",
    "            image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "            landmark_point = []\n",
    "\n",
    "            # Keypoint\n",
    "            for _, landmark in enumerate(landmarks.landmark):\n",
    "                landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "                landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "                # landmark_z = landmark.z\n",
    "\n",
    "                landmark_point.append([landmark_x, landmark_y])\n",
    "\n",
    "            return landmark_point\n",
    "\n",
    "\n",
    "# initialize hand keypoint classifier\n",
    "key_point_classifier = KeyPointClassifier(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bounding_rect(image, landmarks):\n",
    "    # Calculate bounding box from hand landmarks\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_array = np.empty((0, 2), int)\n",
    "\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "\n",
    "        landmark_point = [np.array((landmark_x, landmark_y))]\n",
    "\n",
    "        landmark_array = np.append(landmark_array, landmark_point, axis=0)\n",
    "\n",
    "    x, y, w, h = cv2.boundingRect(landmark_array)\n",
    "\n",
    "    return [x, y, x + w, y + h]\n",
    "\n",
    "def calc_center(brect):\n",
    "    # calculate center of hand bounding box\n",
    "    return (brect[0] + brect[2]) / 2, (brect[1] + brect[3]) / 2\n",
    "\n",
    "def draw_info(image, brect, hand_sign_text = \"\"):\n",
    "    # draw bounding box of hand\n",
    "    # Outer rectangle\n",
    "    cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[3]),\n",
    "                    (0, 0, 0), 3)\n",
    "\n",
    "    # Text\n",
    "    cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[1] - 22),\n",
    "                     (0, 0, 0), -1)\n",
    "    cv2.putText(image, hand_sign_text, (brect[0] + 5, brect[1] - 4),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "def draw_person(image, box):\n",
    "    x0 = int(box[0])\n",
    "    y0 = int(box[1])\n",
    "    x1 = int(box[2])\n",
    "    y1 = int(box[3])\n",
    "\n",
    "    color = (0, 0, 255)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.rectangle(image, (x0, y0), (x1, y1), color, 2)\n",
    "\n",
    "def in_bounding_box(p, bbox):\n",
    "    # check if point is in bounding box\n",
    "    return p[0] >= bbox[0] and p[0] <= bbox[2] and p[1] >= bbox[1] and p[1] <= bbox[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward time: 0.018s | decode time: 0.005s | forward time: 0.017s | decode time: 0.005s | forward time: 0.018s | decode time: 0.005s | forward time: 0.018s | decode time: 0.004s | forward time: 0.018s | decode time: 0.005s | forward time: 0.017s | decode time: 0.005s | forward time: 0.018s | decode time: 0.005s | forward time: 0.018s | decode time: 0.005s | forward time: 0.017s | decode time: 0.005s | forward time: 0.017s | decode time: 0.005s | forward time: 0.017s | decode time: 0.005s | forward time: 0.017s | decode time: 0.005s | forward time: 0.017s | decode time: 0.005s | forward time: 0.017s | decode time: 0.004s | forward time: 0.017s | decode time: 0.005s | forward time: 0.017s | decode time: 0.005s | forward time: 0.018s | decode time: 0.004s | forward time: 0.017s | decode time: 0.005s | "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2d/qlljx9fd3_vckd2wfbdrtq500000gn/T/ipykernel_44824/3418440011.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MediaPipe Hands'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xFF\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m27\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# init values for FPS calculation\n",
    "prev_frame_time = 0\n",
    "new_frame_time = 0\n",
    "\n",
    "\n",
    "# start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "  model_complexity=0,\n",
    "  min_detection_confidence=0.5,\n",
    "  min_tracking_confidence=0.5,\n",
    "  max_num_hands=4) as hands:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      continue\n",
    "    \n",
    "    # flip image for selfie mode\n",
    "    image = cv2.flip(image, 1)\n",
    "    prev_time = time.time()\n",
    "    operations = []\n",
    "    # Process the image in a non-writable way (faster) + convert to RGB to detect hands\n",
    "    img_copy = image.copy()\n",
    "    img_copy.flags.writeable = False\n",
    "    img_copy = cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_copy)\n",
    "  \n",
    "    # if hands detected\n",
    "    if results.multi_hand_landmarks:\n",
    "      hand_centers = []\n",
    "      # for each hand\n",
    "      for hand_landmarks in results.multi_hand_landmarks:\n",
    "        # draw hand keypoins\n",
    "        mp_drawing.draw_landmarks(\n",
    "          image,\n",
    "          hand_landmarks,\n",
    "          mp_hands.HAND_CONNECTIONS,\n",
    "          mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "          mp_drawing_styles.get_default_hand_connections_style())\n",
    "        \n",
    "        # classify keypoints\n",
    "        gesture_idx = key_point_classifier(image, hand_landmarks)\n",
    "        # if sign detected (== 2, \"up\")\n",
    "        if gesture_idx == 2:\n",
    "          # draw bounding box of hand\n",
    "          brect = calc_bounding_rect(image, hand_landmarks)\n",
    "          draw_info(image, brect, \"Sign detected\")\n",
    "          # save center of hand bounding box\n",
    "          hand_center = calc_center(brect)\n",
    "          hand_centers.append(hand_center)\n",
    "      # if the sign is detected (hand_center defined)   \n",
    "      if len(hand_centers) > 1:\n",
    "        # detect persons in the frame\n",
    "        meta, res = predictor.inference(image);\n",
    "        dets = res[0]\n",
    "        bboxes = []\n",
    "        distances = []\n",
    "        for label in dets:\n",
    "          if label == 0:\n",
    "            for bbox in dets[label]:\n",
    "              score = bbox[-1]\n",
    "              if score > 0.35:\n",
    "                rel_hand_centers = []\n",
    "                for hand_center in hand_centers:\n",
    "                  if in_bounding_box(hand_center, bbox[:4]):\n",
    "                    rel_hand_centers.append(hand_center)\n",
    "                if len(rel_hand_centers) > 1:\n",
    "                  draw_person(image, bbox[:4])\n",
    "\n",
    "    # Calculate FPS + show on frame\n",
    "    new_frame_time = time.time()\n",
    "    fps = 1/(new_frame_time-prev_frame_time)\n",
    "    prev_frame_time = new_frame_time\n",
    "    fps = str(int(fps))\n",
    "    cv2.putText(image, fps, (7, 70), cv2.FONT_HERSHEY_SIMPLEX, 3, (100, 255, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('MediaPipe Hands', image)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e1c65c56a63675cdc5d6d3ad906f5d82dbb85d41bce690e359258414f60052e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('yolox')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
