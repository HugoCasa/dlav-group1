{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugo/opt/anaconda3/envs/yolo/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import copy\n",
    "import itertools\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load media pipe drawing solutions\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "# load media pipe hand detection model\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Initialized TensorFlow Lite runtime.\n"
     ]
    }
   ],
   "source": [
    "# load hand keypoints classifier\n",
    "model_path = \"models/keypoint_classifier.tflite\"\n",
    "class KeyPointClassifier(object):\n",
    "    \"\"\"\n",
    "    Classify hand keys points into 8 gestures\n",
    "    \n",
    "    Note: the classification model and class has been taken and refactored from https://github.com/kinivi/tello-gesture-control\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path=\"models/keypoint_classifier.tflite\",\n",
    "        \n",
    "    ):\n",
    "        self.interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "        self.interpreter.allocate_tensors()\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        frame,\n",
    "        hand_landmarks,\n",
    "    ):\n",
    "        # Landmark calculation\n",
    "        landmark_list = self._calc_landmark_list(frame, hand_landmarks)\n",
    "\n",
    "        # Conversion to relative coordinates / normalized coordinates\n",
    "        pre_processed_landmark_list = self._pre_process_landmark(landmark_list)\n",
    "\n",
    "        input_details_tensor_index = self.input_details[0]['index']\n",
    "        self.interpreter.set_tensor(\n",
    "            input_details_tensor_index,\n",
    "            np.array([pre_processed_landmark_list], dtype=np.float32))\n",
    "        self.interpreter.invoke()\n",
    "\n",
    "        output_details_tensor_index = self.output_details[0]['index']\n",
    "\n",
    "        result = self.interpreter.get_tensor(output_details_tensor_index)\n",
    "\n",
    "        result_index = np.argmax(np.squeeze(result))\n",
    "\n",
    "        return result_index\n",
    "    \n",
    "    def _pre_process_landmark(self, landmark_list):\n",
    "        temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "\n",
    "        # Convert to relative coordinates\n",
    "        base_x, base_y = 0, 0\n",
    "        for index, landmark_point in enumerate(temp_landmark_list):\n",
    "            if index == 0:\n",
    "                base_x, base_y = landmark_point[0], landmark_point[1]\n",
    "\n",
    "            temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
    "            temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
    "\n",
    "        # Convert to a one-dimensional list\n",
    "        temp_landmark_list = list(\n",
    "            itertools.chain.from_iterable(temp_landmark_list))\n",
    "\n",
    "        # Normalization\n",
    "        max_value = max(list(map(abs, temp_landmark_list)))\n",
    "\n",
    "        def normalize_(n):\n",
    "            return n / max_value\n",
    "\n",
    "        temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
    "\n",
    "        return temp_landmark_list\n",
    "    \n",
    "    def _calc_landmark_list(self, image, landmarks):\n",
    "            image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "            landmark_point = []\n",
    "\n",
    "            # Keypoint\n",
    "            for _, landmark in enumerate(landmarks.landmark):\n",
    "                landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "                landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "                # landmark_z = landmark.z\n",
    "\n",
    "                landmark_point.append([landmark_x, landmark_y])\n",
    "\n",
    "            return landmark_point\n",
    "\n",
    "\n",
    "# initialize hand keypoint classifier\n",
    "key_point_classifier = KeyPointClassifier(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/hugo/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2022-4-23 torch 1.11.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# load yolo v5 nano model (for person detection)\n",
    "#yolo = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5n.mlmodel') # apple optimized\n",
    "yolo = torch.hub.load('ultralytics/yolov5', 'yolov5n')\n",
    "\n",
    "# limit yolo model to only detect persons\n",
    "yolo.classes = [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bounding_rect(image, landmarks):\n",
    "    # Calculate bounding box from hand landmarks\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_array = np.empty((0, 2), int)\n",
    "\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "\n",
    "        landmark_point = [np.array((landmark_x, landmark_y))]\n",
    "\n",
    "        landmark_array = np.append(landmark_array, landmark_point, axis=0)\n",
    "\n",
    "    x, y, w, h = cv2.boundingRect(landmark_array)\n",
    "\n",
    "    return [x, y, x + w, y + h]\n",
    "\n",
    "def calc_center(brect):\n",
    "    # calculate center of hand bounding box\n",
    "    return (brect[0] + brect[2]) / 2, (brect[1] + brect[3]) / 2\n",
    "\n",
    "def draw_info(image, brect, hand_sign_text = \"\"):\n",
    "    # draw bounding box of hand\n",
    "    # Outer rectangle\n",
    "    cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[3]),\n",
    "                    (0, 0, 0), 3)\n",
    "\n",
    "    # Text\n",
    "    cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[1] - 22),\n",
    "                     (0, 0, 0), -1)\n",
    "    cv2.putText(image, hand_sign_text, (brect[0] + 5, brect[1] - 4),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "def draw_person(image, p):\n",
    "    # draw bounding box of person\n",
    "    cv2.rectangle(image, (int(p.xmin), int(p.ymin)), (int(p.xmax), int(p.ymax)), (0, 0, 255), 2)\n",
    "\n",
    "def in_bounding_box(p, bbox):\n",
    "    # check if point is in bounding box\n",
    "    return p[0] >= bbox[0] and p[0] <= bbox[2] and p[1] >= bbox[1] and p[1] <= bbox[3]\n",
    "\n",
    "def euclidian_distance(p1, p2):\n",
    "    # calculate euclidian distance between two points\n",
    "    return math.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resulting Milestone 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2d/qlljx9fd3_vckd2wfbdrtq500000gn/T/ipykernel_14541/1166299556.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MediaPipe Hands'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xFF\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m27\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# init values for FPS calculation\n",
    "prev_frame_time = 0\n",
    "new_frame_time = 0\n",
    "\n",
    "# start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_hands.Hands(\n",
    "  model_complexity=0,\n",
    "  min_detection_confidence=0.6,\n",
    "  min_tracking_confidence=0.5,\n",
    "  max_num_hands=4) as hands:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      continue\n",
    "    \n",
    "    # flip image for selfie mode\n",
    "    image = cv2.flip(image, 1)\n",
    "\n",
    "    # Process the image in a non-writable way (faster) + convert to RGB to detect hands\n",
    "    img_copy = image.copy()\n",
    "    img_copy.flags.writeable = False\n",
    "    img_copy = cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_copy)\n",
    "\n",
    "    # detect persons in the frame\n",
    "    yolo_pred = yolo(image)\n",
    "    persons = yolo_pred.pandas().xyxy[0]\n",
    "\n",
    "    # if hands detected\n",
    "    if results.multi_hand_landmarks:\n",
    "      hand_centers = []\n",
    "      brects = []\n",
    "      # for each hand\n",
    "      for hand_landmarks in results.multi_hand_landmarks:\n",
    "        # draw hand keypoins\n",
    "        mp_drawing.draw_landmarks(\n",
    "          image,\n",
    "          hand_landmarks,\n",
    "          mp_hands.HAND_CONNECTIONS,\n",
    "          mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "          mp_drawing_styles.get_default_hand_connections_style())\n",
    "        \n",
    "        # classify keypoints\n",
    "        gesture_idx = key_point_classifier(image, hand_landmarks)\n",
    "        # if sign detected (== 2, \"up\")\n",
    "        if gesture_idx == 2:\n",
    "          # calculate bounding box of hand\n",
    "          brect = calc_bounding_rect(image, hand_landmarks)\n",
    "          # save center of hand bounding box\n",
    "          hand_center = calc_center(brect)\n",
    "          hand_centers.append(hand_center)\n",
    "          brects.append(brect)\n",
    "      # if the sign is detected more than once  \n",
    "      if len(hand_centers) > 0:\n",
    "        for _, p in persons.iterrows():\n",
    "          rel_hand_centers = []\n",
    "          # check if at least two hands are in the person bounding box (and discard too close hand detections)\n",
    "          for hand_center in hand_centers:\n",
    "            dup = False\n",
    "            for rhc in rel_hand_centers:\n",
    "              if euclidian_distance(hand_center, rhc) < 50:\n",
    "                dup = True\n",
    "                break\n",
    "            if in_bounding_box(hand_center, [p.xmin, p.ymin, p.xmax, p.ymax]) and not dup:\n",
    "              rel_hand_centers.append(hand_center)\n",
    "              draw_info(image, brects[hand_centers.index(hand_center)], \"Sign detected\")\n",
    "          if len(rel_hand_centers) > 1:\n",
    "            draw_person(image, p)\n",
    "            break\n",
    "\n",
    "    # Calculate FPS + show on frame\n",
    "    new_frame_time = time.time()\n",
    "    fps = 1/(new_frame_time-prev_frame_time)\n",
    "    prev_frame_time = new_frame_time\n",
    "    fps = str(int(fps))\n",
    "    cv2.putText(image, fps, (7, 70), cv2.FONT_HERSHEY_SIMPLEX, 3, (100, 255, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('MediaPipe Hands', image)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5be6a9cfe1a3bb102a1993d1bec0b6710b0b517b60d9164311363fecf04be31"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dlav')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
